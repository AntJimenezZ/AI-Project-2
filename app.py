"""
Interfaz web para clasificaci√≥n de im√°genes y reconocimiento de voz
Usa Gradio para crear una interfaz sencilla con dos pesta√±as
"""

import gradio as gr
import tensorflow as tf
import numpy as np
import librosa
from PIL import Image
import os
import cv2
import tempfile

# ============================================
# CONFIGURACI√ìN DE RUTAS
# ============================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODELS_DIR = os.path.join(BASE_DIR, "models")

# Rutas de modelos de clasificaci√≥n de im√°genes
RESNET_MODEL_PATH = os.path.join(MODELS_DIR, "best_model_resnet50.keras")
MOBILENET_MODEL_PATH = os.path.join(MODELS_DIR, "best_model_mobilenetv2.keras")

# Ruta del modelo de speech-to-text (cuando est√© disponible)
SPEECH_MODEL_PATH = os.path.join(MODELS_DIR, "best_model.keras")

# Clases para clasificaci√≥n de im√°genes
CLASS_NAMES = ['Gato', 'Perro']

# Par√°metros de audio (coinciden con el entrenamiento de parte2)
SR = 16000
N_FFT = 512
HOP_LENGTH = 160
N_MELS = 80
MAX_AUDIO_SECONDS = 8.0

# Vocabulario usado en el entrenamiento CTC (parte2)
CHARS = list("abcdefghijklmnopqrstuvwxyz√±√°√©√≠√≥√∫√º'.,?¬°! ")
CHAR_TO_IDX = {c: i + 1 for i, c in enumerate(CHARS)}  # 0 es blank/padding
IDX_TO_CHAR = {i: c for c, i in CHAR_TO_IDX.items()}

# ============================================
# CARGAR MODELOS DE CLASIFICACI√ìN DE IM√ÅGENES
# ============================================
print("Cargando modelos de clasificaci√≥n de im√°genes...")

try:
    model_resnet = tf.keras.models.load_model(RESNET_MODEL_PATH)
    print("‚úì Modelo ResNet50 cargado correctamente")
except Exception as e:
    print(f"‚úó Error al cargar ResNet50: {e}")
    model_resnet = None

try:
    model_mobilenet = tf.keras.models.load_model(MOBILENET_MODEL_PATH)
    print("‚úì Modelo MobileNetV2 cargado correctamente")
except Exception as e:
    print(f"‚úó Error al cargar MobileNetV2: {e}")
    model_mobilenet = None

# Modelo de speech-to-text
speech_model = None
try:
    if os.path.exists(SPEECH_MODEL_PATH):
        speech_model = tf.keras.models.load_model(SPEECH_MODEL_PATH, compile=False)
        print("‚úì Modelo de speech-to-text cargado correctamente")
    else:
        print("‚ö†Ô∏è Modelo de speech-to-text no encontrado en 'models/best_model.keras'")
except Exception as e:
    print(f"‚úó Error al cargar modelo de speech-to-text: {e}")
    speech_model = None

# ============================================
# FUNCIONES PARA CLASIFICACI√ìN DE IM√ÅGENES
# ============================================

def preprocess_image(image):
    """
    Preprocesa la imagen para los modelos de clasificaci√≥n
    Args:
        image: PIL Image
    Returns:
        numpy array normalizado y redimensionado
    """
    # Convertir a RGB si es necesario
    if image.mode != 'RGB':
        image = image.convert('RGB')
    
    # Redimensionar a 224x224 (tama√±o est√°ndar para ResNet50 y MobileNetV2)
    image = image.resize((224, 224))
    
    # Convertir a array numpy
    img_array = np.array(image)
    
    # Normalizar a [0, 1]
    img_array = img_array.astype(np.float32) / 255.0
    
    # Agregar dimensi√≥n de batch
    img_array = np.expand_dims(img_array, axis=0)
    
    return img_array


# ============================================
# FUNCIONES PARA AUDIO Y DECODIFICACI√ìN CTC
# ============================================


def load_wav(path, sr=SR):
    """Carga y recorta audio a la duraci√≥n m√°xima."""
    x, _ = librosa.load(path, sr=sr)
    if x.shape[0] > sr * MAX_AUDIO_SECONDS:
        x = x[: int(sr * MAX_AUDIO_SECONDS)]
    return x


def wav_to_log_mel(x):
    """Convierte audio a espectrograma log-mel normalizado."""
    S = librosa.feature.melspectrogram(y=x, sr=SR, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS)
    S_db = librosa.power_to_db(S, ref=np.max)
    S_db = (S_db + 80.0) / 80.0  # normalizar a ~[0,1]
    return S_db.T  # frames x mel


def greedy_decode(logits, logit_lengths):
    decoded = tf.nn.ctc_greedy_decoder(tf.transpose(logits, [1, 0, 2]), sequence_length=logit_lengths)[0][0]
    dense = tf.sparse.to_dense(decoded, default_value=0).numpy()
    texts = []
    for row in dense:
        s = ""
        for idx in row:
            if idx == 0:
                continue
            s += IDX_TO_CHAR.get(int(idx), '')
        texts.append(s)
    return texts


def classify_image(image):
    """
    Clasifica una imagen usando ambos modelos
    Args:
        image: PIL Image
    Returns:
        dict con resultados de ambos modelos
    """
    if image is None:
        return "Por favor, sube una imagen"
    
    try:
        # Preprocesar imagen
        img_array = preprocess_image(image)
        
        results = {}
        
        # Predicci√≥n con ResNet50
        if model_resnet is not None:
            pred_resnet = model_resnet.predict(img_array, verbose=0)
            pred_class_resnet = np.argmax(pred_resnet[0])
            confidence_resnet = float(pred_resnet[0][pred_class_resnet]) * 100
            
            results["ResNet50"] = {
                "Predicci√≥n": CLASS_NAMES[pred_class_resnet],
                "Confianza": f"{confidence_resnet:.2f}%"
            }
            
            # Crear distribuci√≥n de probabilidades para ResNet50
            resnet_probs = {CLASS_NAMES[i]: float(pred_resnet[0][i]) for i in range(len(CLASS_NAMES))}
        else:
            results["ResNet50"] = {"Error": "Modelo no disponible"}
            resnet_probs = None
        
        # Predicci√≥n con MobileNetV2
        if model_mobilenet is not None:
            pred_mobilenet = model_mobilenet.predict(img_array, verbose=0)
            pred_class_mobilenet = np.argmax(pred_mobilenet[0])
            confidence_mobilenet = float(pred_mobilenet[0][pred_class_mobilenet]) * 100
            
            results["MobileNetV2"] = {
                "Predicci√≥n": CLASS_NAMES[pred_class_mobilenet],
                "Confianza": f"{confidence_mobilenet:.2f}%"
            }
            
            # Crear distribuci√≥n de probabilidades para MobileNetV2
            mobilenet_probs = {CLASS_NAMES[i]: float(pred_mobilenet[0][i]) for i in range(len(CLASS_NAMES))}
        else:
            results["MobileNetV2"] = {"Error": "Modelo no disponible"}
            mobilenet_probs = None
        
        # Formatear resultados como texto
        output_text = "RESULTADOS DE CLASIFICACI√ìN\n\n"
        
        if model_resnet is not None:
            output_text += f"ResNet50:\n"
            output_text += f"   Predicci√≥n: {results['ResNet50']['Predicci√≥n']}\n"
            output_text += f"   Confianza: {results['ResNet50']['Confianza']}\n\n"
        
        if model_mobilenet is not None:
            output_text += f"MobileNetV2:\n"
            output_text += f"   Predicci√≥n: {results['MobileNetV2']['Predicci√≥n']}\n"
            output_text += f"   Confianza: {results['MobileNetV2']['Confianza']}\n"
        
        return output_text, resnet_probs, mobilenet_probs
        
    except Exception as e:
        return f"Error al procesar la imagen: {str(e)}", None, None


def classify_video(video_path):
    """
    Procesa un video extrayendo 1 frame por segundo y clasificando cada uno con ambos modelos.
    Retorna un an√°lisis con detecciones por modelo y un video con anotaciones.
    
    Args:
        video_path: ruta al archivo de video
    Returns:
        tupla con (reporte_texto, video_anotado)
    """
    if video_path is None:
        return "Por favor, sube un video", None
    
    try:
        # Abrir el video
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            return "Error: No se pudo abrir el video", None
        
        # Obtener propiedades del video
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        if fps == 0:
            fps = 30  # valor por defecto
        
        duration = total_frames / fps if fps > 0 else 0
        
        # Configurar escritor de video de salida
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        temp_output_path = os.path.join(tempfile.gettempdir(), "video_clasificado.mp4")
        out = cv2.VideoWriter(temp_output_path, fourcc, fps, (frame_width, frame_height))
        
        # Variables para an√°lisis
        detections_mobilenet = []
        detections_resnet = []
        frame_count = 0
        processed_frames = 0
        
        # Procesar frames
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            
            # Extraer 1 frame por segundo (cada fps frames)
            if frame_count % fps == 0 or frame_count == 1:
                processed_frames += 1
                
                # Convertir BGR a RGB para PIL
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                pil_image = Image.fromarray(frame_rgb)
                
                # Clasificar la imagen con ambos modelos
                try:
                    img_array = preprocess_image(pil_image)
                    
                    # Predicci√≥n con MobileNetV2
                    mobilenet_class = None
                    mobilenet_conf = 0
                    if model_mobilenet is not None:
                        pred = model_mobilenet.predict(img_array, verbose=0)
                        pred_class = np.argmax(pred[0])
                        confidence = float(pred[0][pred_class]) * 100
                        mobilenet_class = CLASS_NAMES[pred_class]
                        mobilenet_conf = confidence
                        
                        detections_mobilenet.append({
                            "frame": processed_frames,
                            "segundo": frame_count / fps,
                            "clase": mobilenet_class,
                            "confianza": f"{confidence:.2f}%"
                        })
                    
                    # Predicci√≥n con ResNet50
                    resnet_class = None
                    resnet_conf = 0
                    if model_resnet is not None:
                        pred = model_resnet.predict(img_array, verbose=0)
                        pred_class = np.argmax(pred[0])
                        confidence = float(pred[0][pred_class]) * 100
                        resnet_class = CLASS_NAMES[pred_class]
                        resnet_conf = confidence
                        
                        detections_resnet.append({
                            "frame": processed_frames,
                            "segundo": frame_count / fps,
                            "clase": resnet_class,
                            "confianza": f"{confidence:.2f}%"
                        })
                    
                    # Anotaciones en el video (mostrar ambos modelos)
                    y_offset = 40
                    
                    if model_mobilenet is not None and mobilenet_class:
                        color_mb = (0, 255, 0) if mobilenet_class == "Gato" else (255, 0, 0)
                        label_mb = f"MobileNet: {mobilenet_class} ({mobilenet_conf:.1f}%)"
                        cv2.putText(frame, label_mb, (20, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 
                                  0.8, color_mb, 2, cv2.LINE_AA)
                        y_offset += 35
                    
                    if model_resnet is not None and resnet_class:
                        color_rn = (0, 255, 0) if resnet_class == "Gato" else (255, 0, 0)
                        label_rn = f"ResNet50: {resnet_class} ({resnet_conf:.1f}%)"
                        cv2.putText(frame, label_rn, (20, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 
                                  0.8, color_rn, 2, cv2.LINE_AA)
                        y_offset += 35
                    
                    # Mostrar tiempo
                    cv2.putText(frame, f"Tiempo: {frame_count/fps:.1f}s", (20, y_offset), 
                              cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)
                
                except Exception as e:
                    print(f"Error procesando frame {processed_frames}: {e}")
            
            # Escribir frame en el video de salida
            out.write(frame)
        
        # Liberar recursos
        cap.release()
        out.release()
        
        # Generar reporte
        report = "üìä AN√ÅLISIS DE CLASIFICACI√ìN DE VIDEO\n"
        report += f"{'='*70}\n\n"
        report += f"üìπ Informaci√≥n del video:\n"
        report += f"   ‚Ä¢ Duraci√≥n: {duration:.2f} segundos\n"
        report += f"   ‚Ä¢ FPS: {fps}\n"
        report += f"   ‚Ä¢ Total de frames: {total_frames}\n"
        report += f"   ‚Ä¢ Frames procesados: {processed_frames}\n"
        report += f"   ‚Ä¢ Resoluci√≥n: {frame_width}x{frame_height}\n\n"
        
        # An√°lisis MobileNetV2
        if detections_mobilenet:
            report += f"{'='*70}\n"
            report += f"ü§ñ RESULTADOS - MobileNetV2\n"
            report += f"{'='*70}\n"
            gatos_mb = sum(1 for d in detections_mobilenet if d["clase"] == "Gato")
            perros_mb = sum(1 for d in detections_mobilenet if d["clase"] == "Perro")
            report += f"   ‚Ä¢ Gatos detectados: {gatos_mb}\n"
            report += f"   ‚Ä¢ Perros detectados: {perros_mb}\n\n"
            report += f"{'Frame':<8} {'Segundo':<10} {'Clase':<10} {'Confianza':<12}\n"
            report += f"{'-'*50}\n"
            for det in detections_mobilenet:
                report += f"{det['frame']:<8} {det['segundo']:<10.2f} {det['clase']:<10} {det['confianza']:<12}\n"
        
        # An√°lisis ResNet50
        if detections_resnet:
            report += f"\n{'='*70}\n"
            report += f"ü§ñ RESULTADOS - ResNet50\n"
            report += f"{'='*70}\n"
            gatos_rn = sum(1 for d in detections_resnet if d["clase"] == "Gato")
            perros_rn = sum(1 for d in detections_resnet if d["clase"] == "Perro")
            report += f"   ‚Ä¢ Gatos detectados: {gatos_rn}\n"
            report += f"   ‚Ä¢ Perros detectados: {perros_rn}\n\n"
            report += f"{'Frame':<8} {'Segundo':<10} {'Clase':<10} {'Confianza':<12}\n"
            report += f"{'-'*50}\n"
            for det in detections_resnet:
                report += f"{det['frame']:<8} {det['segundo']:<10.2f} {det['clase']:<10} {det['confianza']:<12}\n"
        
        report += f"\n{'='*70}\n"
        report += f"‚úÖ Video procesado y guardado con anotaciones\n"
        report += f"{'='*70}\n"
        
        return report, temp_output_path
        
    except Exception as e:
        return f"Error al procesar el video: {str(e)}", None



# ============================================
# FUNCI√ìN PARA SPEECH-TO-TEXT (PLACEHOLDER)
# ============================================

def transcribe_audio(audio):
    """Transcribe audio a texto usando el modelo CTC entrenado."""
    if audio is None:
        return "Por favor, graba o sube un audio"

    # Asegurar que el modelo est√© cargado
    global speech_model
    if speech_model is None:
        if not os.path.exists(SPEECH_MODEL_PATH):
            return "Modelo de speech-to-text no disponible. Guarda best_model.keras en la carpeta models/"
        try:
            speech_model = tf.keras.models.load_model(SPEECH_MODEL_PATH, compile=False)
        except Exception as e:
            return f"No se pudo cargar el modelo de speech-to-text: {e}"

    try:
        # audio puede ser una ruta (str) o una tupla (sr, datos)
        if isinstance(audio, str):
            wav = load_wav(audio)
        else:
            # audio llega como (sample_rate, np.ndarray)
            sr_in, data = audio
            if sr_in != SR:
                data = librosa.resample(np.array(data, dtype=np.float32), orig_sr=sr_in, target_sr=SR)
            wav = data

        feat = wav_to_log_mel(wav)  # (frames, 80)
        if feat.shape[0] < 2:
            return "Audio demasiado corto para transcripci√≥n"

        x = np.expand_dims(feat, axis=0)  # (1, frames, 80)
        logits = speech_model.predict(x, verbose=0)

        # Longitud despu√©s del stride=2 de la primera conv del modelo
        logit_len = feat.shape[0] // 2
        input_len = tf.constant([logit_len], dtype=tf.int32)

        preds = greedy_decode(logits, input_len)
        return preds[0] if preds else ""

    except Exception as e:
        return f"Error al transcribir el audio: {str(e)}"


# ============================================
# CREAR INTERFAZ CON GRADIO
# ============================================

# Crear interfaz con pesta√±as
with gr.Blocks(title="Clasificador y Transcriptor") as app:
    
    gr.Markdown(
        """
        # Sistema de Clasificaci√≥n y Transcripci√≥n
        ### Modelos de Deep Learning para im√°genes y audio
        """
    )
    
    with gr.Tabs():
        
        # ============================================
        # PESTA√ëA 1: CLASIFICACI√ìN DE IM√ÅGENES
        # ============================================
        with gr.Tab("Clasificaci√≥n de Im√°genes"):
            gr.Markdown(
                """
                Sube una imagen de un **gato** o **perro** para clasificarla.
                Se utilizar√°n dos modelos diferentes para comparar resultados.
                """
            )
            
            with gr.Row():
                with gr.Column(scale=1):
                    image_input = gr.Image(
                        type="pil",
                        label="Sube una imagen",
                        height=400
                    )
                    classify_btn = gr.Button("Clasificar Imagen", variant="primary", size="lg")
                
                with gr.Column(scale=1):
                    output_text = gr.Textbox(
                        label="Resultados",
                        lines=10,
                        max_lines=15
                    )
            
            with gr.Row():
                with gr.Column():
                    gr.Markdown("### Distribuci√≥n de probabilidades - ResNet50")
                    resnet_label = gr.Label(num_top_classes=2, label="ResNet50")
                
                with gr.Column():
                    gr.Markdown("### Distribuci√≥n de probabilidades - MobileNetV2")
                    mobilenet_label = gr.Label(num_top_classes=2, label="MobileNetV2")
            
            # Ejemplos de im√°genes (opcional)
            gr.Examples(
                examples=[],  # Puedes agregar rutas de ejemplo aqu√≠
                inputs=image_input,
                label="Ejemplos"
            )
            
            # Conectar el bot√≥n con la funci√≥n
            classify_btn.click(
                fn=classify_image,
                inputs=image_input,
                outputs=[output_text, resnet_label, mobilenet_label]
            )
        
        # ============================================
        # PESTA√ëA 1.5: CLASIFICACI√ìN DE VIDEOS
        # ============================================
        with gr.Tab("Clasificaci√≥n de Videos"):
            gr.Markdown(
                """
                Sube un video para extraer frames y clasificar gatos o perros.
                Se extrae 1 frame por segundo y se genera un video anotado con los resultados.
                """
            )
            
            with gr.Row():
                with gr.Column(scale=1):
                    video_input = gr.Video(
                        label="Sube un video",
                        format="mp4"
                    )
                    video_btn = gr.Button("Procesar Video", variant="primary", size="lg")
                
                with gr.Column(scale=1):
                    video_report = gr.Textbox(
                        label="An√°lisis del Video",
                        lines=15,
                        max_lines=20
                    )
            
            with gr.Row():
                video_output = gr.Video(
                    label="Video Clasificado (Con Anotaciones)",
                    format="mp4"
                )
            
            gr.Markdown(
                """
                **C√≥mo funciona:**
                1. Sube un archivo de video en formato MP4
                2. El sistema extrae 1 frame por segundo
                3. Cada frame se clasifica usando MobileNetV2
                4. Se genera un video con anotaciones mostrando la clase detectada y confianza
                5. Se proporciona un an√°lisis detallado de todas las detecciones
                
                **Leyenda de colores:**
                - üü¢ Verde: Gato detectado
                - üîµ Azul: Perro detectado
                """
            )
            
            # Conectar el bot√≥n con la funci√≥n
            video_btn.click(
                fn=classify_video,
                inputs=video_input,
                outputs=[video_report, video_output]
            )
        
        # ============================================
        # PESTA√ëA 2: SPEECH-TO-TEXT
        # ============================================
        with gr.Tab("Reconocimiento de Voz"):
            gr.Markdown(
                """
                Graba o sube un archivo de audio en espa√±ol para transcribirlo a texto.
                """
            )
            
            with gr.Row():
                with gr.Column():
                    audio_input = gr.Audio(
                        sources=["microphone", "upload"],
                        type="filepath",
                        label="Graba o sube un audio"
                    )
                    transcribe_btn = gr.Button("Transcribir Audio", variant="primary", size="lg")
                
                with gr.Column():
                    transcription_output = gr.Textbox(
                        label="Transcripci√≥n",
                        lines=10,
                        max_lines=15,
                        placeholder="La transcripci√≥n aparecer√° aqu√≠..."
                    )
            
            gr.Markdown(
                """
                ---
                **Nota:** Esta funcionalidad requiere que primero entrenes el modelo en `parte2.ipynb` 
                y guardes el archivo `best_model.keras` en la carpeta `models/`.
                """
            )
            
            # Conectar el bot√≥n con la funci√≥n
            transcribe_btn.click(
                fn=transcribe_audio,
                inputs=audio_input,
                outputs=transcription_output
            )
    
    # Pie de p√°gina
    gr.Markdown(
        """
        ---
        **Nota:** Los modelos est√°n optimizados para im√°genes de 224x224 p√≠xeles y audios de hasta 8 segundos.
        """
    )

# ============================================
# LANZAR LA APLICACI√ìN
# ============================================

if __name__ == "__main__":
    print("\n" + "="*50)
    print("Iniciando la aplicaci√≥n...")
    print("="*50 + "\n")
    

    app.launch(
        server_name="127.0.0.1",
        server_port=7860,
        share=False,
        inbrowser=True 
    )
