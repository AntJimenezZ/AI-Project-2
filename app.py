"""
Interfaz web para clasificaciÃ³n de imÃ¡genes y reconocimiento de voz
Usa Gradio para crear una interfaz sencilla con dos pestaÃ±as
"""

import gradio as gr
import tensorflow as tf
import numpy as np
import librosa
from PIL import Image
import os
import cv2
import tempfile

# ============================================
# CONFIGURACIÃ“N DE RUTAS
# ============================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODELS_DIR = os.path.join(BASE_DIR, "models")

# Rutas de modelos de clasificaciÃ³n de imÃ¡genes
RESNET_MODEL_PATH = os.path.join(MODELS_DIR, "best_model_resnet50.keras")
MOBILENET_MODEL_PATH = os.path.join(MODELS_DIR, "best_model_mobilenetv2.keras")

# Ruta del modelo de speech-to-text (cuando estÃ© disponible)
SPEECH_MODEL_PATH = os.path.join(MODELS_DIR, "best_model.keras")

# Clases para clasificaciÃ³n de imÃ¡genes
CLASS_NAMES = ['Gato', 'Perro']

# ParÃ¡metros de audio (coinciden con el entrenamiento de parte2)
SR = 16000
N_FFT = 512
HOP_LENGTH = 160
N_MELS = 80
MAX_AUDIO_SECONDS = 8.0

# Vocabulario usado en el entrenamiento CTC (parte2)
CHARS = list("abcdefghijklmnopqrstuvwxyzÃ±Ã¡Ã©Ã­Ã³ÃºÃ¼'.,?Â¡! ")
CHAR_TO_IDX = {c: i + 1 for i, c in enumerate(CHARS)}  # 0 es blank/padding
IDX_TO_CHAR = {i: c for c, i in CHAR_TO_IDX.items()}

# ============================================
# CARGAR MODELOS DE CLASIFICACIÃ“N DE IMÃGENES
# ============================================
print("Cargando modelos de clasificaciÃ³n de imÃ¡genes...")

try:
    model_resnet = tf.keras.models.load_model(RESNET_MODEL_PATH)
    print("âœ“ Modelo ResNet50 cargado correctamente")
except Exception as e:
    print(f"âœ— Error al cargar ResNet50: {e}")
    model_resnet = None

try:
    model_mobilenet = tf.keras.models.load_model(MOBILENET_MODEL_PATH)
    print("âœ“ Modelo MobileNetV2 cargado correctamente")
except Exception as e:
    print(f"âœ— Error al cargar MobileNetV2: {e}")
    model_mobilenet = None

# Modelo de speech-to-text
speech_model = None
try:
    if os.path.exists(SPEECH_MODEL_PATH):
        speech_model = tf.keras.models.load_model(SPEECH_MODEL_PATH, compile=False)
        print("âœ“ Modelo de speech-to-text cargado correctamente")
    else:
        print("âš ï¸ Modelo de speech-to-text no encontrado en 'models/best_model.keras'")
except Exception as e:
    print(f"âœ— Error al cargar modelo de speech-to-text: {e}")
    speech_model = None

# ============================================
# FUNCIONES PARA CLASIFICACIÃ“N DE IMÃGENES
# ============================================

def preprocess_image(image):
    """
    Preprocesa la imagen para los modelos de clasificaciÃ³n
    Args:
        image: PIL Image
    Returns:
        numpy array normalizado y redimensionado
    """
    # Convertir a RGB si es necesario
    if image.mode != 'RGB':
        image = image.convert('RGB')
    
    # Redimensionar a 224x224 (tamaÃ±o estÃ¡ndar para ResNet50 y MobileNetV2)
    image = image.resize((224, 224))
    
    # Convertir a array numpy
    img_array = np.array(image)
    
    # Normalizar a [0, 1]
    img_array = img_array.astype(np.float32) / 255.0
    
    # Agregar dimensiÃ³n de batch
    img_array = np.expand_dims(img_array, axis=0)
    
    return img_array


# ============================================
# FUNCIONES PARA AUDIO Y DECODIFICACIÃ“N CTC
# ============================================


def load_wav(path, sr=SR):
    """Carga y recorta audio a la duraciÃ³n mÃ¡xima."""
    x, _ = librosa.load(path, sr=sr)
    if x.shape[0] > sr * MAX_AUDIO_SECONDS:
        x = x[: int(sr * MAX_AUDIO_SECONDS)]
    return x


def wav_to_log_mel(x):
    """Convierte audio a espectrograma log-mel normalizado."""
    S = librosa.feature.melspectrogram(y=x, sr=SR, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS)
    S_db = librosa.power_to_db(S, ref=np.max)
    S_db = (S_db + 80.0) / 80.0  # normalizar a ~[0,1]
    return S_db.T  # frames x mel


def greedy_decode(logits, logit_lengths):
    decoded = tf.nn.ctc_greedy_decoder(tf.transpose(logits, [1, 0, 2]), sequence_length=logit_lengths)[0][0]
    dense = tf.sparse.to_dense(decoded, default_value=0).numpy()
    texts = []
    for row in dense:
        s = ""
        for idx in row:
            if idx == 0:
                continue
            s += IDX_TO_CHAR.get(int(idx), '')
        texts.append(s)
    return texts


def classify_image(image):
    """
    Clasifica una imagen usando ambos modelos
    Args:
        image: PIL Image
    Returns:
        dict con resultados de ambos modelos
    """
    if image is None:
        return "Por favor, sube una imagen"
    
    try:
        # Preprocesar imagen
        img_array = preprocess_image(image)
        
        results = {}
        
        # PredicciÃ³n con ResNet50
        if model_resnet is not None:
            pred_resnet = model_resnet.predict(img_array, verbose=0)
            pred_class_resnet = np.argmax(pred_resnet[0])
            confidence_resnet = float(pred_resnet[0][pred_class_resnet]) * 100
            
            results["ResNet50"] = {
                "PredicciÃ³n": CLASS_NAMES[pred_class_resnet],
                "Confianza": f"{confidence_resnet:.2f}%"
            }
            
            # Crear distribuciÃ³n de probabilidades para ResNet50
            resnet_probs = {CLASS_NAMES[i]: float(pred_resnet[0][i]) for i in range(len(CLASS_NAMES))}
        else:
            results["ResNet50"] = {"Error": "Modelo no disponible"}
            resnet_probs = None
        
        # PredicciÃ³n con MobileNetV2
        if model_mobilenet is not None:
            pred_mobilenet = model_mobilenet.predict(img_array, verbose=0)
            pred_class_mobilenet = np.argmax(pred_mobilenet[0])
            confidence_mobilenet = float(pred_mobilenet[0][pred_class_mobilenet]) * 100
            
            results["MobileNetV2"] = {
                "PredicciÃ³n": CLASS_NAMES[pred_class_mobilenet],
                "Confianza": f"{confidence_mobilenet:.2f}%"
            }
            
            # Crear distribuciÃ³n de probabilidades para MobileNetV2
            mobilenet_probs = {CLASS_NAMES[i]: float(pred_mobilenet[0][i]) for i in range(len(CLASS_NAMES))}
        else:
            results["MobileNetV2"] = {"Error": "Modelo no disponible"}
            mobilenet_probs = None
        
        # Formatear resultados como texto
        output_text = "RESULTADOS DE CLASIFICACIÃ“N\n\n"
        
        if model_resnet is not None:
            output_text += f"ResNet50:\n"
            output_text += f"   PredicciÃ³n: {results['ResNet50']['PredicciÃ³n']}\n"
            output_text += f"   Confianza: {results['ResNet50']['Confianza']}\n\n"
        
        if model_mobilenet is not None:
            output_text += f"MobileNetV2:\n"
            output_text += f"   PredicciÃ³n: {results['MobileNetV2']['PredicciÃ³n']}\n"
            output_text += f"   Confianza: {results['MobileNetV2']['Confianza']}\n"
        
        return output_text, resnet_probs, mobilenet_probs
        
    except Exception as e:
        return f"Error al procesar la imagen: {str(e)}", None, None


def classify_video(video_path):
    """
    Procesa un video extrayendo 1 frame por segundo y clasificando cada uno con ambos modelos.
    Utiliza suavizado de predicciones para mayor estabilidad.
    Retorna un anÃ¡lisis con detecciones por modelo y un video con anotaciones.
    
    Args:
        video_path: ruta al archivo de video
    Returns:
        tupla con (reporte_texto, video_anotado)
    """
    if video_path is None:
        return "Por favor, sube un video", None
    
    try:
        # Abrir el video
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            return "Error: No se pudo abrir el video", None
        
        # Obtener propiedades del video
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        if fps == 0:
            fps = 30  # valor por defecto
        
        duration = total_frames / fps if fps > 0 else 0
        
        # Configurar escritor de video de salida
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        temp_output_path = os.path.join(tempfile.gettempdir(), "video_clasificado.mp4")
        out = cv2.VideoWriter(temp_output_path, fourcc, fps, (frame_width, frame_height))
        
        # Variables para anÃ¡lisis
        detections_mobilenet = []
        detections_resnet = []
        frame_count = 0
        processed_frames = 0
        
        # Variables para suavizado de predicciones
        last_mobilenet_class = None
        last_mobilenet_conf = 0
        last_resnet_class = None
        last_resnet_conf = 0
        frames_since_update_mb = 0
        frames_since_update_rn = 0
        max_frames_display = fps * 2  # Mostrar predicciÃ³n durante 2 segundos
        
        # Procesar frames
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            
            # Extraer 1 frame por segundo (cada fps frames)
            if frame_count % fps == 0 or frame_count == 1:
                processed_frames += 1
                
                # Convertir BGR a RGB para PIL
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                pil_image = Image.fromarray(frame_rgb)
                
                # Clasificar la imagen con ambos modelos
                try:
                    img_array = preprocess_image(pil_image)
                    
                    # PredicciÃ³n con MobileNetV2
                    if model_mobilenet is not None:
                        pred = model_mobilenet.predict(img_array, verbose=0)
                        pred_class = np.argmax(pred[0])
                        confidence = float(pred[0][pred_class]) * 100
                        mobilenet_class = CLASS_NAMES[pred_class]
                        
                        # Solo actualizar si la confianza es suficientemente alta (>50%)
                        if confidence > 50:
                            last_mobilenet_class = mobilenet_class
                            last_mobilenet_conf = confidence
                            frames_since_update_mb = 0
                            
                            detections_mobilenet.append({
                                "frame": processed_frames,
                                "segundo": frame_count / fps,
                                "clase": mobilenet_class,
                                "confianza": f"{confidence:.2f}%"
                            })
                    
                    # PredicciÃ³n con ResNet50
                    if model_resnet is not None:
                        pred = model_resnet.predict(img_array, verbose=0)
                        pred_class = np.argmax(pred[0])
                        confidence = float(pred[0][pred_class]) * 100
                        resnet_class = CLASS_NAMES[pred_class]
                        
                        # Solo actualizar si la confianza es suficientemente alta (>50%)
                        if confidence > 50:
                            last_resnet_class = resnet_class
                            last_resnet_conf = confidence
                            frames_since_update_rn = 0
                            
                            detections_resnet.append({
                                "frame": processed_frames,
                                "segundo": frame_count / fps,
                                "clase": resnet_class,
                                "confianza": f"{confidence:.2f}%"
                            })
                
                except Exception as e:
                    print(f"Error procesando frame {processed_frames}: {e}")
            
            # Dibujar predicciones en TODOS los frames (no solo en los procesados)
            # Esto crea un efecto de "mantener visible" la predicciÃ³n durante 2 segundos
            y_offset = 50
            
            if last_mobilenet_class and frames_since_update_mb < max_frames_display:
                color_mb = (0, 255, 0) if last_mobilenet_class == "Gato" else (255, 0, 0)
                label_mb = f"MobileNetV2: {last_mobilenet_class} ({last_mobilenet_conf:.1f}%)"
                
                # Dibujar rectÃ¡ngulo de fondo para mejor legibilidad
                font = cv2.FONT_HERSHEY_SIMPLEX
                font_scale = 1.0
                thickness = 2
                text_size = cv2.getTextSize(label_mb, font, font_scale, thickness)[0]
                
                # RectÃ¡ngulo de fondo
                cv2.rectangle(frame, (15, y_offset - text_size[1] - 10), 
                            (25 + text_size[0], y_offset + 10), color_mb, -1)
                
                # Texto blanco
                cv2.putText(frame, label_mb, (20, y_offset), font, font_scale, 
                          (255, 255, 255), thickness, cv2.LINE_AA)
                y_offset += 50
            
            if last_resnet_class and frames_since_update_rn < max_frames_display:
                color_rn = (0, 255, 0) if last_resnet_class == "Gato" else (255, 0, 0)
                label_rn = f"ResNet50: {last_resnet_class} ({last_resnet_conf:.1f}%)"
                
                # Dibujar rectÃ¡ngulo de fondo para mejor legibilidad
                font = cv2.FONT_HERSHEY_SIMPLEX
                font_scale = 1.0
                thickness = 2
                text_size = cv2.getTextSize(label_rn, font, font_scale, thickness)[0]
                
                # RectÃ¡ngulo de fondo
                cv2.rectangle(frame, (15, y_offset - text_size[1] - 10), 
                            (25 + text_size[0], y_offset + 10), color_rn, -1)
                
                # Texto blanco
                cv2.putText(frame, label_rn, (20, y_offset), font, font_scale, 
                          (255, 255, 255), thickness, cv2.LINE_AA)
                y_offset += 50
            
            # Mostrar tiempo
            cv2.putText(frame, f"Tiempo: {frame_count/fps:.1f}s", (20, y_offset), 
                      cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)
            
            # Incrementar contadores
            frames_since_update_mb += 1
            frames_since_update_rn += 1
            
            # Escribir frame en el video de salida
            out.write(frame)
        
        # Liberar recursos
        cap.release()
        out.release()
        
        # Generar reporte
        report = "ðŸ“Š ANÃLISIS DE CLASIFICACIÃ“N DE VIDEO\n"
        report += f"{'='*70}\n\n"
        report += f"ðŸ“¹ InformaciÃ³n del video:\n"
        report += f"   â€¢ DuraciÃ³n: {duration:.2f} segundos\n"
        report += f"   â€¢ FPS: {fps}\n"
        report += f"   â€¢ Total de frames: {total_frames}\n"
        report += f"   â€¢ Frames procesados (1 por segundo): {processed_frames}\n"
        report += f"   â€¢ ResoluciÃ³n: {frame_width}x{frame_height}\n\n"
        
        report += f"âš™ï¸ ConfiguraciÃ³n de procesamiento:\n"
        report += f"   â€¢ Umbral de confianza mÃ­nima: 50%\n"
        report += f"   â€¢ DuraciÃ³n de visualizaciÃ³n: 2 segundos por detecciÃ³n\n"
        report += f"   â€¢ Suavizado: Activado\n\n"
        
        # AnÃ¡lisis MobileNetV2
        if detections_mobilenet:
            report += f"{'='*70}\n"
            report += f"ðŸ¤– RESULTADOS - MobileNetV2\n"
            report += f"{'='*70}\n"
            gatos_mb = sum(1 for d in detections_mobilenet if d["clase"] == "Gato")
            perros_mb = sum(1 for d in detections_mobilenet if d["clase"] == "Perro")
            report += f"   â€¢ Gatos detectados: {gatos_mb}\n"
            report += f"   â€¢ Perros detectados: {perros_mb}\n"
            report += f"   â€¢ Total detecciones: {len(detections_mobilenet)}\n\n"
            report += f"{'Frame':<8} {'Segundo':<10} {'Clase':<10} {'Confianza':<12}\n"
            report += f"{'-'*50}\n"
            for det in detections_mobilenet:
                report += f"{det['frame']:<8} {det['segundo']:<10.2f} {det['clase']:<10} {det['confianza']:<12}\n"
        else:
            report += f"{'='*70}\n"
            report += f"ðŸ¤– RESULTADOS - MobileNetV2\n"
            report += f"{'='*70}\n"
            report += f"   âš ï¸ No se realizaron detecciones con confianza > 50%\n\n"
        
        # AnÃ¡lisis ResNet50
        if detections_resnet:
            report += f"\n{'='*70}\n"
            report += f"ðŸ¤– RESULTADOS - ResNet50\n"
            report += f"{'='*70}\n"
            gatos_rn = sum(1 for d in detections_resnet if d["clase"] == "Gato")
            perros_rn = sum(1 for d in detections_resnet if d["clase"] == "Perro")
            report += f"   â€¢ Gatos detectados: {gatos_rn}\n"
            report += f"   â€¢ Perros detectados: {perros_rn}\n"
            report += f"   â€¢ Total detecciones: {len(detections_resnet)}\n\n"
            report += f"{'Frame':<8} {'Segundo':<10} {'Clase':<10} {'Confianza':<12}\n"
            report += f"{'-'*50}\n"
            for det in detections_resnet:
                report += f"{det['frame']:<8} {det['segundo']:<10.2f} {det['clase']:<10} {det['confianza']:<12}\n"
        else:
            report += f"\n{'='*70}\n"
            report += f"ðŸ¤– RESULTADOS - ResNet50\n"
            report += f"{'='*70}\n"
            report += f"   âš ï¸ No se realizaron detecciones con confianza > 50%\n\n"
        
        report += f"\n{'='*70}\n"
        report += f"âœ… Video procesado y guardado con anotaciones\n"
        report += f"{'='*70}\n"
        
        return report, temp_output_path
        
    except Exception as e:
        return f"Error al procesar el video: {str(e)}", None



# ============================================
# FUNCIÃ“N PARA SPEECH-TO-TEXT (PLACEHOLDER)
# ============================================

def transcribe_audio(audio):
    """Transcribe audio a texto usando el modelo CTC entrenado."""
    if audio is None:
        return "Por favor, graba o sube un audio"

    # Asegurar que el modelo estÃ© cargado
    global speech_model
    if speech_model is None:
        if not os.path.exists(SPEECH_MODEL_PATH):
            return "Modelo de speech-to-text no disponible. Guarda best_model.keras en la carpeta models/"
        try:
            speech_model = tf.keras.models.load_model(SPEECH_MODEL_PATH, compile=False)
        except Exception as e:
            return f"No se pudo cargar el modelo de speech-to-text: {e}"

    try:
        # audio puede ser una ruta (str) o una tupla (sr, datos)
        if isinstance(audio, str):
            wav = load_wav(audio)
        else:
            # audio llega como (sample_rate, np.ndarray)
            sr_in, data = audio
            if sr_in != SR:
                data = librosa.resample(np.array(data, dtype=np.float32), orig_sr=sr_in, target_sr=SR)
            wav = data

        feat = wav_to_log_mel(wav)  # (frames, 80)
        if feat.shape[0] < 2:
            return "Audio demasiado corto para transcripciÃ³n"

        x = np.expand_dims(feat, axis=0)  # (1, frames, 80)
        logits = speech_model.predict(x, verbose=0)

        # Longitud despuÃ©s del stride=2 de la primera conv del modelo
        logit_len = feat.shape[0] // 2
        input_len = tf.constant([logit_len], dtype=tf.int32)

        preds = greedy_decode(logits, input_len)
        return preds[0] if preds else ""

    except Exception as e:
        return f"Error al transcribir el audio: {str(e)}"


# ============================================
# CREAR INTERFAZ CON GRADIO
# ============================================

# Crear interfaz con pestaÃ±as
with gr.Blocks(title="Clasificador y Transcriptor") as app:
    
    gr.Markdown(
        """
        # Sistema de ClasificaciÃ³n y TranscripciÃ³n
        ### Modelos de Deep Learning para imÃ¡genes y audio
        """
    )
    
    with gr.Tabs():
        
        # ============================================
        # PESTAÃ‘A 1: CLASIFICACIÃ“N DE IMÃGENES
        # ============================================
        with gr.Tab("ClasificaciÃ³n de ImÃ¡genes"):
            gr.Markdown(
                """
                Sube una imagen de un **gato** o **perro** para clasificarla.
                Se utilizarÃ¡n dos modelos diferentes para comparar resultados.
                """
            )
            
            with gr.Row():
                with gr.Column(scale=1):
                    image_input = gr.Image(
                        type="pil",
                        label="Sube una imagen",
                        height=400
                    )
                    classify_btn = gr.Button("Clasificar Imagen", variant="primary", size="lg")
                
                with gr.Column(scale=1):
                    output_text = gr.Textbox(
                        label="Resultados",
                        lines=10,
                        max_lines=15
                    )
            
            with gr.Row():
                with gr.Column():
                    gr.Markdown("### DistribuciÃ³n de probabilidades - ResNet50")
                    resnet_label = gr.Label(num_top_classes=2, label="ResNet50")
                
                with gr.Column():
                    gr.Markdown("### DistribuciÃ³n de probabilidades - MobileNetV2")
                    mobilenet_label = gr.Label(num_top_classes=2, label="MobileNetV2")
            
            # Ejemplos de imÃ¡genes (opcional)
            gr.Examples(
                examples=[],  # Puedes agregar rutas de ejemplo aquÃ­
                inputs=image_input,
                label="Ejemplos"
            )
            
            # Conectar el botÃ³n con la funciÃ³n
            classify_btn.click(
                fn=classify_image,
                inputs=image_input,
                outputs=[output_text, resnet_label, mobilenet_label]
            )
        
        # ============================================
        # PESTAÃ‘A 1.5: CLASIFICACIÃ“N DE VIDEOS
        # ============================================
        with gr.Tab("ClasificaciÃ³n de Videos"):
            gr.Markdown(
                """
                Sube un video para extraer frames y clasificar gatos o perros.
                Se extrae 1 frame por segundo y se genera un video anotado con los resultados.
                """
            )
            
            with gr.Row():
                with gr.Column(scale=1):
                    video_input = gr.Video(
                        label="Sube un video",
                        format="mp4"
                    )
                    video_btn = gr.Button("Procesar Video", variant="primary", size="lg")
                
                with gr.Column(scale=1):
                    video_report = gr.Textbox(
                        label="AnÃ¡lisis del Video",
                        lines=15,
                        max_lines=20
                    )
            
            with gr.Row():
                video_output = gr.Video(
                    label="Video Clasificado (Con Anotaciones)",
                    format="mp4"
                )
            
            gr.Markdown(
                """
                **CÃ³mo funciona:**
                1. Sube un archivo de video en formato MP4
                2. El sistema extrae 1 frame por segundo
                3. Cada frame se clasifica usando MobileNetV2
                4. Se genera un video con anotaciones mostrando la clase detectada y confianza
                5. Se proporciona un anÃ¡lisis detallado de todas las detecciones
                
                **Leyenda de colores:**
                - ðŸŸ¢ Verde: Gato detectado
                - ðŸ”µ Azul: Perro detectado
                """
            )
            
            # Conectar el botÃ³n con la funciÃ³n
            video_btn.click(
                fn=classify_video,
                inputs=video_input,
                outputs=[video_report, video_output]
            )
        
        # ============================================
        # PESTAÃ‘A 2: SPEECH-TO-TEXT
        # ============================================
        with gr.Tab("Reconocimiento de Voz"):
            gr.Markdown(
                """
                Graba o sube un archivo de audio en espaÃ±ol para transcribirlo a texto.
                """
            )
            
            with gr.Row():
                with gr.Column():
                    audio_input = gr.Audio(
                        sources=["microphone", "upload"],
                        type="filepath",
                        label="Graba o sube un audio"
                    )
                    transcribe_btn = gr.Button("Transcribir Audio", variant="primary", size="lg")
                
                with gr.Column():
                    transcription_output = gr.Textbox(
                        label="TranscripciÃ³n",
                        lines=10,
                        max_lines=15,
                        placeholder="La transcripciÃ³n aparecerÃ¡ aquÃ­..."
                    )
            
            gr.Markdown(
                """
                ---
                **Nota:** Esta funcionalidad requiere que primero entrenes el modelo en `parte2.ipynb` 
                y guardes el archivo `best_model.keras` en la carpeta `models/`.
                """
            )
            
            # Conectar el botÃ³n con la funciÃ³n
            transcribe_btn.click(
                fn=transcribe_audio,
                inputs=audio_input,
                outputs=transcription_output
            )
    
    # Pie de pÃ¡gina
    gr.Markdown(
        """
        ---
        **Nota:** Los modelos estÃ¡n optimizados para imÃ¡genes de 224x224 pÃ­xeles y audios de hasta 8 segundos.
        """
    )

# ============================================
# LANZAR LA APLICACIÃ“N
# ============================================

if __name__ == "__main__":
    print("\n" + "="*50)
    print("Iniciando la aplicaciÃ³n...")
    print("="*50 + "\n")
    

    app.launch(
        server_name="127.0.0.1",
        server_port=7860,
        share=False,
        inbrowser=True 
    )
